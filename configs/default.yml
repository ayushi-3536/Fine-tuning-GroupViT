data:
  batch_size: 128
  pin_memory: true
  num_workers: 2
  # Thomas said it should be at least about 5-10x your batch size; beyond that,
  # the differences become academic.
  shuffle_buffer: 10000
  seed: ${train.seed}
  dataset:
    meta:
      gcc3m:
        type: img_txt_pair
        path: local_data/gcc3m_shards
        prefix: gcc-train-{000000..00436}.tar
        length: 2891445
      gcc12m:
        type: img_txt_pair
        path: local_data/gcc12m_shards
        prefix: gcc-conceptual-12m-{000000..001943}.tar
        length: 11156203
      yfcc14m:
        type: img_txt_pair
        path: local_data/yfcc14m_shards
        prefix: yfcc14m-{000000..001888}.tar
        length: 14615499
      redcap12m:
        type: img_txt_pair
        path: local_data/redcap12m_shards
        prefix: redcap12m-{000000..001211}.tar
        length: 11866987
      imagenet:
        type: img_cls_pair
        path: /misc/lmbraid21/sharmaa/imagenet/shards
        prefix: imagenet-val-{000000..000006}.tar
        length: 50000
      coco_train:
        type: img_txt_pair
        path: /misc/lmbraid21/sharmaa/train_coco/mscoco1
        prefix: train_coco_{000000..000059}.tar
        length: 600000


    train:
      #- gcc3m
      #- gcc12m
      #- yfcc14m
      - coco_train
    val:
      - imagenet

  img_aug:
    deit_aug: true
    img_size:  384
    img_scale: [0.08, 1.0]
    interpolation: bilinear
    color_jitter: 0.4
    auto_augment: 'rand-m9-mstd0.5-inc1'
    re_prob: 0.25
    re_mode: 'pixel'
    re_count: 1
  text_aug:
    max_seq_len: 77
    multi_label: 5
    template_set: 'subset'
    #word type could be noun, noun phrase or both
    word_type: 'noun_phrase'
    pre_generated_nouns: false
    #if not set then true
    generate_prompt: true
    select_nouns: 
    #generate prompt would trump generate_prompt_for_np. Only if generate_prompt 
    #is true generate_prompt_for_np would be considered
    generate_prompt_for_np: true
    #if not set then true
    with_caption: true
    #if not set then always true
    max_length_fixed: true
    use_pad_token: true

train:
  start_epoch: 0
  epochs: 30
  warmup_epochs: 1
  base_lr: 1.6e-3
  weight_decay: 0.05
  warmup_lr: 4e-6
  min_lr: 4e-5
  lr_scaling: 0.01 # GroupVIT linearly scales the learning rate.
  #If this field is set, default scaling will be replaced by scaling with this unit
  clip_grad: 5.0
  accumulation_steps: 0
  amp_opt_level: O1
  seed: 0

  lr_scheduler:
    name: cosine
    t_mul: 1
    cycle_limit: 2
    decay_epochs: 12

  finetune:
    only_grouping: true #Finetune both grouping blocks
    only_grouping2: false #Finetune only second grouping block
    only_mlp_projectors: false #Finetune only mlp projectors
    freeze_text_encoder: false
    only_img_projector: false

  optimizer:
    name: adamw
    eps: 1e-8
    betas: [0.9, 0.999]


evaluate:
  eval_only: false
  eval_freq: 1
  task:
    #- cls
    - seg
  cls:
    save_best: true
    template: subset
  seg:
    save_best: true
    #cfg: segmentation/configs/_base_/datasets/coco.py
    cfg: segmentation/configs/_base_/datasets/pascal_voc12.py
    #cfg: segmentation/configs/_base_/datasets/pascal_context.py    
    #cfg: segmentation/configs/_base_/datasets/ade20k.py
    template: simple
    opts: []

checkpoint:
  auto_resume: true
  resume: '' #'/misc/student/sharmaa/checkpoints/group_vit_gcc_yfcc_30e-879422e0.pth'
  freq: 1
  max_kept: -1
  save_freq: 1
  key_mapper: '' #'/home/sharmaa/groupvit/GroupViT/configs/key_mapper_gs3.yml'
  interpolate_pos_embedding: true

model_name: '' # display name in the logger
output: ???
wandb_output: '/misc/lmbraid21/sharmaa/wandb'
wandb_id: tph3vjv8
tag: default
print_freq: 1
seed: 0
wandb: true
local_rank: ???
vis: []
