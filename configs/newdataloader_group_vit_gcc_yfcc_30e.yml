_base_: 'default.yml'
data:
  batch_size: 128
  pin_memory: true
  num_workers: 4
  precompute_pad_mask: true
  pad_word: '<PAD>'
  # Thomas said it should be at least about 5-10x your batch size; beyond that,
  # the differences become academic.
  seed: ${train.seed}
  train:
    root_dir: [
                '/misc/lmbraid21/sharmaa/coco_stuff164k/images/train2017',
              ]
    meta_file: [
                '/misc/student/sharmaa/groupvit/OVSegmentor/scripts/coco_meta.json',
              ]
    use_dali: True
    input_size: 224
    test_resize: 448
    image_reader:
        type: pil
    sampler:
        type: distributed_epoch
    transforms:
        type: STANDARD
    fseek: True
    use_ranked: False
  
  # val:
  #     - imagenet

model:
  type: MultiLabelContrastiveRefined
  ### This disable parallelization of the model
  debugging: False
  use_group_token_entropy_loss: True
  use_label_entropy_loss: False
  use_tiered_entropy_loss: False
  use_pad_token: True

  img_encoder:
    type: GroupViT
    img_size:  384
    embed_dim: 384
    num_heads: [6, 6, 6]
    depths: [6, 3, 3]
    num_group_tokens: [64, 8, 0]
    num_output_groups: [64, 8]
    drop_rate: 0.0
    drop_path_rate: 0.1
  text_encoder:
    type: TextTransformer
    context_length: 77
    width: 256
    layers: 12
    vocab_size: 49408
  contrast_temperature: 0.07
  proj_num_layers: 2
  output_dim: 256
  multi_label: ${data.text_aug.multi_label}
  
wandb: true
